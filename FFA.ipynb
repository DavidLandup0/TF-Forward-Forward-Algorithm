{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1aad3a5",
   "metadata": {},
   "source": [
    "### The FFA Algorithm\n",
    "\n",
    "In [The Forward-Forward Algorithm: Some Preliminary\n",
    "Investigations](https://www.cs.toronto.edu/~hinton/FFA13.pdf) - Geoffrey Hinton proposes an alternative to backpropagation, called the Forward-Forward Algorithm.\n",
    "\n",
    "In FFA - the input is passed twice through the network in a forward-fashion, while no backward pass happens. Instead of a loss function, a \"goodness\" function is used to evaluate the predictions of the network. One of the forward passes is a \"positive\" pass, while the other is the \"negative\" pass.\n",
    "\n",
    "- The positive pass aims to maximize the goodness on real data (actual input features and labels)\n",
    "- The negative pass aims to minimize the goodness on fake data (corrupt input features and wrong labels)\n",
    "\n",
    "While many goodness measures can be used, the paper proposes a sum of the squares of activations of intermediate ReLUs and a negative sum of the squares of activations of intermediate ReLUs.\n",
    "\n",
    "The way data is corrupted for the negative pass is also variable - there are many ways to corrupt data. Creating masks (and inverted masks) and mixing up training data by multiplying two input feature sets with the masks and combining them into a hybrid image are used.\n",
    "\n",
    "The following is a minimal TensorFlow-based implementation of the Forward-Forward Algorithm.\n",
    "\n",
    "**Note:** The implementation is a work-in-progress and will periodically be updated. It's released now to (hopefully) start a discussion in this lane.\n",
    "\n",
    "### TODO:\n",
    "\n",
    "- Creating hybrid negative data (i.e. tf.reduce_sum(masks*input_image_pairs))\n",
    "- Accounting for negative data\n",
    "- Figuring out how to extract intermediate activations in TF\n",
    "\n",
    "\n",
    "### Why Try To Move from Backpropagation?\n",
    "\n",
    "Backpropagation is great! Though, it doesn't seem like it's what's happening in the brain, in which a much faster operation is taking place. In a classical deep learning workflow, we train networks in three basic steps:\n",
    "\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- Updating weights\n",
    "\n",
    "During the forward pass and backward pass, *no learning is hapenning*. Additionally, backpropagation requires the knowledge of the structure of the network during the forward pass, and is tricky for physical implementation[1](https://www.nature.com/articles/s41467-022-35216-2).\n",
    "\n",
    "Moving away from backpropagation is likely to allow us to train networks without knowledge of the structure (or physical system), and potentially with higher efficiency.\n",
    "\n",
    "### FFA Doesn't Replace Backpropagation\n",
    "\n",
    "FFA, while a great step towards a broader solution, doesn't currently replace backprop and isn't meant to replace it. Some limitations and unknowns include:\n",
    "\n",
    "- Doesn't work with networks that make use of weight-sharing (such as CNNs)\n",
    "- Isn't clear how well it scales\n",
    "- Requires potentially expensive creation of negative data\n",
    "- Isn't faster than backprop (because two forward-passes are done)\n",
    "- What makes for a good \"goodness\" function? Which activation functions yield the best results?\n",
    "\n",
    "\n",
    "Further announced work by Geoffrey Hinton will tackle these questions. In the meantime, making the research accessible, and *inciting discussions* can help possibly derive further methods inspired by FFA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b890e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 16:45:51.581178: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e406328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795fe89",
   "metadata": {},
   "source": [
    "# Classic NN with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd45304",
   "metadata": {},
   "source": [
    "Let's start with a classic Neural Net using a \"hand-made\" SGD implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1ade7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 16:45:58.881159: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(32)\n",
    "test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f616a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class FF(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(FF, self).__init__()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.d1 = layers.Dense(128)\n",
    "        self.relu = layers.Activation('relu', name='relu')\n",
    "        self.d2 = layers.Dense(10, activation=\"softmax\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.d2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953b867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80f806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/tgk9npg5163832yrf5b5h47r0000gn/T/ipykernel_62008/2884861351.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  new_weights = model.trainable_weights - 0.01 * np.array(grads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 2.3725\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 1.0972\n",
      "Seen so far: 6432 samples\n",
      "Training loss (for one batch) at step 400: 0.6239\n",
      "Seen so far: 12832 samples\n",
      "Training loss (for one batch) at step 600: 0.6025\n",
      "Seen so far: 19232 samples\n",
      "Training loss (for one batch) at step 800: 0.3762\n",
      "Seen so far: 25632 samples\n",
      "Training loss (for one batch) at step 1000: 0.3119\n",
      "Seen so far: 32032 samples\n",
      "Training loss (for one batch) at step 1200: 0.2801\n",
      "Seen so far: 38432 samples\n",
      "Training loss (for one batch) at step 1400: 0.6508\n",
      "Seen so far: 44832 samples\n",
      "Training loss (for one batch) at step 1600: 0.4695\n",
      "Seen so far: 51232 samples\n",
      "Training loss (for one batch) at step 1800: 0.1336\n",
      "Seen so far: 57632 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        \n",
    "        # Update trainable paramms using Gradient Descent\n",
    "        new_weights = model.trainable_weights - 0.01 * np.array(grads)\n",
    "        for index, variable in enumerate(model.trainable_weights):\n",
    "            variable.assign(new_weights[index])\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4866100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ff\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  100480    \n",
      "                                                                 \n",
      " relu (Activation)           multiple                  0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913ce3b",
   "metadata": {},
   "source": [
    "# FFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c51f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Cannot use Subclassing because there is no inherennt \n",
    "# structure - the network is an arbitrary `call()` method.\n",
    "# Thus, relu's output has no inbound nodes and there's no `.output`\n",
    "class FF_subclass(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.d1 = layers.Dense(128)\n",
    "        self.relu = layers.Activation('relu', name='relu')\n",
    "        self.d2 = layers.Dense(10, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, labels):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.d1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.d2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db8bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannot use Functional because KerasTensors object has no attribute '_id'\n",
    "# which is required for `tape.gradient()`.\n",
    "# Functional models don't support gradient outputs as per: https://github.com/tensorflow/tensorflow/issues/46194#issuecomment-827036146\n",
    "def FF_func():\n",
    "    inputs = layers.Input(shape=[28, 28, 1])\n",
    "    x = layers.Flatten()(inputs)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.Activation('relu', name='relu')(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ebdd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional Subclassing to the rescue?\n",
    "# Clear graph structure + custom training loop and access to gradients?\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class FF(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        inputs = layers.Input(shape=[28, 28, 1])\n",
    "        x = layers.Flatten()(inputs)\n",
    "        x = layers.Dense(128)(x)\n",
    "        x = layers.Activation('relu', name='relu')(x)\n",
    "        output = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "        super().__init__(\n",
    "            inputs={\n",
    "                \"inputs\": inputs,\n",
    "            },\n",
    "            outputs={\n",
    "                \"output\": output,\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad89aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6926d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are also passed as inputs to the forward calls.\n",
    "# This is a temporary implementation.\n",
    "train = tf.data.Dataset.from_tensor_slices(((x_train, y_train), y_train)).shuffle(len(x_train)).batch(32)\n",
    "test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a972050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'goodness' function - with just a single layer\n",
    "# returning the sum of the squares of the ReLU activations\n",
    "def goodness(model, mode):\n",
    "    relu_activations = model.get_layer('relu').output\n",
    "    if mode=='pos':\n",
    "        return tf.reduce_sum(tf.math.square(relu_activations))\n",
    "    else:\n",
    "        return -tf.reduce_sum(tf.math.square(relu_activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ff26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't work for the same reason the Functional API won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10deb380",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    for step, ((x_batch_train, y_batch_real), y_batch_fake) in enumerate(train):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            positive = model(x_batch_train, y_batch_real)\n",
    "            negative = model(x_batch_train, y_batch_fake)\n",
    "            \n",
    "            # \"Loss\" is the \"goodness\" of the hidden layers.\n",
    "            # Positive pass should maximize goodness, negative pass should minimize goodness.\n",
    "            positive_goodness = goodness(model, 'pos')\n",
    "            negative_goodness = goodness(model, 'neg')\n",
    "\n",
    "        grads_positive = tape.gradient(positive_goodness, model.trainable_weights)\n",
    "        grads_negative = tape.gradient(negative_goodness, model.trainable_weights)\n",
    "        \n",
    "        # Maximize and minimize goodness\n",
    "        model.trainable_weights = model.trainable_weights + lr * np.array(positive_goodness)\n",
    "        model.trainable_weights = model.trainable_weights - lr * np.array(negative_goodness)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb48e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e035604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
